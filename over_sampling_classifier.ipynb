{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 功能简介\n",
    "\n",
    "在有标签数据集中，正例数目17000左右，负例数目1709个，所以存在严重的不平衡问题，所以我们尝试解决这个问题。\n",
    "\n",
    "为了解决这个问题，我们需要处理不平衡数据，本py文件使用的是过采样的方法，使用的SMOTE，RandomOverSampler和ADASYN。\n",
    "\n",
    "特征选择使用的是RFECV（由featureSelectionBasic.ipynb得到）\n",
    "\n",
    "参考文章：https://beckernick.github.io/oversampling-modeling/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_extraction import DictVectorizer  \n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "from imblearn.over_sampling import SMOTE,ADASYN,RandomOverSampler\n",
    "import pickle\n",
    "import json\n",
    "from copy import deepcopy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1、获取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = pymysql.Connect(\n",
    "    host=\"localhost\",\n",
    "    port=3306,\n",
    "    user=\"root\",\n",
    "    passwd=\"root\",\n",
    "    charset=\"utf8\",\n",
    "    db=\"project_researchers\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train_data: (17844, 21)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17844 entries, 0 to 17843\n",
      "Data columns (total 21 columns):\n",
      "bys_cn                17608 non-null float64\n",
      "hindex_cn             17707 non-null float64\n",
      "a_paper               17844 non-null int64\n",
      "b_paper               17844 non-null int64\n",
      "c_paper               17844 non-null int64\n",
      "papernum2017          17844 non-null int64\n",
      "papernum2016          17844 non-null int64\n",
      "papernum2015          17844 non-null int64\n",
      "papernum2014          17844 non-null int64\n",
      "papernum2013          17844 non-null int64\n",
      "num_journal           17844 non-null int64\n",
      "num_conference        17844 non-null int64\n",
      "degree                17774 non-null float64\n",
      "pagerank              17774 non-null float64\n",
      "degree_centrality     17774 non-null float64\n",
      "diff_year             17774 non-null float64\n",
      "coauthors_top10000    17844 non-null int64\n",
      "coauthors_top20000    17844 non-null int64\n",
      "coauthors_top30000    17844 non-null int64\n",
      "category              17844 non-null int64\n",
      "label                 17844 non-null int64\n",
      "dtypes: float64(6), int64(15)\n",
      "memory usage: 2.9 MB\n",
      "train_data.info(): None\n"
     ]
    }
   ],
   "source": [
    "def get_train_data(connection):\n",
    "    \"\"\"\n",
    "    查询训练数据，包括特征和标签\n",
    "    :param connection:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    sql_select = \"\"\"\n",
    "     SELECT bys_cn, hindex_cn,a_conf+a_journal as a_paper, b_conf + b_journal as b_paper,c_conf + c_journal as c_paper,papernum2017, papernum2016, papernum2015, papernum2014, papernum2013,num_journal,num_conference, project_num, degree, pagerank,degree_centrality,last_year - first_year as diff_year , coauthors_top10000, coauthors_top20000, coauthors_top30000, category, label \n",
    "     FROM classifier_isTeacher_xgbc WHERE label = 1 and teac_id > 174 and category is not null\n",
    "     UNION ALL\n",
    "     SELECT bys_cn, hindex_cn,a_conf+a_journal as a_paper, b_conf + b_journal as b_paper,c_conf + c_journal as c_paper,papernum2017, papernum2016, papernum2015, papernum2014, papernum2013,num_journal,num_conference, project_num, degree, pagerank,degree_centrality,last_year - first_year as diff_year , coauthors_top10000, coauthors_top20000, coauthors_top30000, category, label \n",
    "     FROM classifier_isTeacher_xgbc WHERE label = 0 and teac_id > 64438 and category is not null\n",
    "    \"\"\"\n",
    "    df = pd.read_sql_query(sql_select, connection)\n",
    "    all_features = ['bys_cn', 'hindex_cn', 'a_paper', 'b_paper', 'c_paper', 'papernum2017', 'papernum2016', 'papernum2015', 'papernum2014', 'papernum2013', 'num_journal', 'num_conference',  'degree', 'pagerank', 'degree_centrality', 'diff_year', 'coauthors_top10000', 'coauthors_top20000', 'coauthors_top30000', 'category', 'label']\n",
    "    data = df[all_features]\n",
    "    return data\n",
    "\n",
    "train_data = get_train_data(connection)\n",
    "print(\"shape of train_data:\", train_data.shape)\n",
    "print(\"train_data.info():\", train_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of test_data: (850, 21)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 850 entries, 0 to 849\n",
      "Data columns (total 21 columns):\n",
      "bys_cn                834 non-null float64\n",
      "hindex_cn             850 non-null int64\n",
      "a_paper               850 non-null int64\n",
      "b_paper               850 non-null int64\n",
      "c_paper               850 non-null int64\n",
      "papernum2017          850 non-null int64\n",
      "papernum2016          850 non-null int64\n",
      "papernum2015          850 non-null int64\n",
      "papernum2014          850 non-null int64\n",
      "papernum2013          850 non-null int64\n",
      "num_journal           850 non-null int64\n",
      "num_conference        850 non-null int64\n",
      "degree                849 non-null float64\n",
      "pagerank              849 non-null float64\n",
      "degree_centrality     849 non-null float64\n",
      "diff_year             849 non-null float64\n",
      "coauthors_top10000    850 non-null int64\n",
      "coauthors_top20000    850 non-null int64\n",
      "coauthors_top30000    850 non-null int64\n",
      "category              850 non-null int64\n",
      "label                 850 non-null int64\n",
      "dtypes: float64(5), int64(16)\n",
      "memory usage: 139.5 KB\n",
      "test_data.info(): None\n"
     ]
    }
   ],
   "source": [
    "def get_test_data(connection):\n",
    "    \"\"\"\n",
    "    查询测试数据，包括特征和标签\n",
    "    :param connection:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    sql_select = \"\"\"\n",
    "     SELECT bys_cn, hindex_cn,a_conf+a_journal as a_paper, b_conf + b_journal as b_paper,c_conf + c_journal as c_paper,papernum2017, papernum2016, papernum2015, papernum2014, papernum2013,num_journal,num_conference, project_num, degree, pagerank,degree_centrality,last_year - first_year as diff_year , coauthors_top10000, coauthors_top20000, coauthors_top30000, category, label \n",
    "     FROM classifier_isTeacher_xgbc WHERE label = 1 and teac_id <= 174 and category is not null\n",
    "     UNION ALL\n",
    "     SELECT bys_cn, hindex_cn,a_conf+a_journal as a_paper, b_conf + b_journal as b_paper,c_conf + c_journal as c_paper,papernum2017, papernum2016, papernum2015, papernum2014, papernum2013,num_journal,num_conference, project_num, degree, pagerank,degree_centrality,last_year - first_year as diff_year , coauthors_top10000, coauthors_top20000, coauthors_top30000, category, label \n",
    "     FROM classifier_isTeacher_xgbc WHERE label = 0 and teac_id <= 64438 and category is not null\n",
    "    \"\"\"\n",
    "    df = pd.read_sql_query(sql_select, connection)\n",
    "    all_features = ['bys_cn', 'hindex_cn', 'a_paper', 'b_paper', 'c_paper', 'papernum2017', 'papernum2016', 'papernum2015', 'papernum2014', 'papernum2013', 'num_journal', 'num_conference',  'degree', 'pagerank', 'degree_centrality', 'diff_year', 'coauthors_top10000', 'coauthors_top20000', 'coauthors_top30000', 'category', 'label']\n",
    "    data = df[all_features]\n",
    "    return data\n",
    "\n",
    "test_data = get_test_data(connection)\n",
    "print(\"shape of test_data:\", test_data.shape)\n",
    "print(\"test_data.info():\", test_data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train_data:: (17539, 21)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 17539 entries, 0 to 17843\n",
      "Data columns (total 21 columns):\n",
      "bys_cn                17539 non-null float64\n",
      "hindex_cn             17539 non-null float64\n",
      "a_paper               17539 non-null int64\n",
      "b_paper               17539 non-null int64\n",
      "c_paper               17539 non-null int64\n",
      "papernum2017          17539 non-null int64\n",
      "papernum2016          17539 non-null int64\n",
      "papernum2015          17539 non-null int64\n",
      "papernum2014          17539 non-null int64\n",
      "papernum2013          17539 non-null int64\n",
      "num_journal           17539 non-null int64\n",
      "num_conference        17539 non-null int64\n",
      "degree                17539 non-null float64\n",
      "pagerank              17539 non-null float64\n",
      "degree_centrality     17539 non-null float64\n",
      "diff_year             17539 non-null float64\n",
      "coauthors_top10000    17539 non-null int64\n",
      "coauthors_top20000    17539 non-null int64\n",
      "coauthors_top30000    17539 non-null int64\n",
      "category              17539 non-null int64\n",
      "label                 17539 non-null int64\n",
      "dtypes: float64(6), int64(15)\n",
      "memory usage: 2.9 MB\n",
      "train_data.info():: None\n",
      "shape of test_data:: (833, 21)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 833 entries, 0 to 849\n",
      "Data columns (total 21 columns):\n",
      "bys_cn                833 non-null float64\n",
      "hindex_cn             833 non-null int64\n",
      "a_paper               833 non-null int64\n",
      "b_paper               833 non-null int64\n",
      "c_paper               833 non-null int64\n",
      "papernum2017          833 non-null int64\n",
      "papernum2016          833 non-null int64\n",
      "papernum2015          833 non-null int64\n",
      "papernum2014          833 non-null int64\n",
      "papernum2013          833 non-null int64\n",
      "num_journal           833 non-null int64\n",
      "num_conference        833 non-null int64\n",
      "degree                833 non-null float64\n",
      "pagerank              833 non-null float64\n",
      "degree_centrality     833 non-null float64\n",
      "diff_year             833 non-null float64\n",
      "coauthors_top10000    833 non-null int64\n",
      "coauthors_top20000    833 non-null int64\n",
      "coauthors_top30000    833 non-null int64\n",
      "category              833 non-null int64\n",
      "label                 833 non-null int64\n",
      "dtypes: float64(5), int64(16)\n",
      "memory usage: 143.2 KB\n",
      "test_data.info():: None\n",
      "y_test_label:: Counter({0: 683, 1: 150})\n"
     ]
    }
   ],
   "source": [
    "# 对缺失值进行处理\n",
    "# Method1：直接将含有缺失字段的值去掉\n",
    "train_data = train_data.dropna()\n",
    "print(\"shape of train_data::\", train_data.shape)\n",
    "print(\"train_data.info()::\", train_data.info())\n",
    "test_data = test_data.dropna()\n",
    "print(\"shape of test_data::\", test_data.shape)\n",
    "print(\"test_data.info()::\", test_data.info())\n",
    "print(\"y_test_label::\", Counter(test_data[\"label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** (17539, 20)\n",
      "17539   20\n",
      "info of X_train: (17539, 22)\n",
      "y_train:: Counter({1: 16551, 0: 988})\n",
      "833   20\n",
      "info of X_test: (833, 22)\n",
      "y_test:: Counter({0: 683, 1: 150})\n"
     ]
    }
   ],
   "source": [
    "# 将category变成离散值（object），并将train_data变成向量（离散值变成one-hot）\n",
    "\n",
    "# training data\n",
    "train_data[['category']] = train_data[['category']].astype(object)\n",
    "y_train = train_data['label']\n",
    "X_train = train_data.drop(columns=['label'])\n",
    "print(\"****\", X_train.shape)\n",
    "\n",
    "vec = DictVectorizer()   \n",
    "\n",
    "# 因为fDictVectorizer().fit_transform()需要的参数时list of dict，所以这里将其转化\n",
    "headers_train = list(X_train.columns)\n",
    "value_df_train = X_train.values\n",
    "feature_list_train = []\n",
    "for value_train in value_df_train:\n",
    "    feature_dict = {}\n",
    "    for i in range(0, len(headers_train)):\n",
    "        if headers_train[i]=='category':\n",
    "            feature_dict[headers_train[i]] = str(value_train[i])\n",
    "        else:\n",
    "            feature_dict[headers_train[i]] = value_train[i]\n",
    "    feature_list_train.append(feature_dict)\n",
    "print(len(feature_list_train), ' ', len(feature_list_train[0]))\n",
    "\n",
    "X_train = vec.fit_transform(feature_list_train)\n",
    "print(\"info of X_train:\", X_train.shape)\n",
    "print(\"y_train::\", Counter(y_train))\n",
    "\n",
    "# testing data\n",
    "test_data[['category']] = test_data[['category']].astype(object)\n",
    "y_test = test_data['label']\n",
    "X_test = test_data.drop(columns=['label'])\n",
    "\n",
    "headers_test = list(X_test.columns)\n",
    "value_df_test = X_test.values\n",
    "# print(headers)\n",
    "# print(value_df.shape)\n",
    "feature_list_test = []\n",
    "for value_test in value_df_test:\n",
    "    feature_dict = {}\n",
    "    for i in range(0, len(headers_test)):\n",
    "        if headers_test[i]=='category':\n",
    "            feature_dict[headers_test[i]] = str(value_test[i])\n",
    "        else:\n",
    "            feature_dict[headers_test[i]] = value_test[i]\n",
    "    feature_list_test.append(feature_dict)\n",
    "print(len(feature_list_test), ' ', len(feature_list_test[0]))\n",
    "\n",
    "X_test = vec.transform(feature_list_test)\n",
    "print(\"info of X_test:\", X_test.shape)\n",
    "print(\"y_test::\", Counter(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3、获取需要预测的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of data_test: (181057, 20)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 181057 entries, 0 to 181056\n",
      "Data columns (total 20 columns):\n",
      "bys_cn                176811 non-null float64\n",
      "hindex_cn             180624 non-null float64\n",
      "a_paper               181057 non-null int64\n",
      "b_paper               181057 non-null int64\n",
      "c_paper               181057 non-null int64\n",
      "papernum2017          181057 non-null int64\n",
      "papernum2016          181057 non-null int64\n",
      "papernum2015          181057 non-null int64\n",
      "papernum2014          181057 non-null int64\n",
      "papernum2013          181057 non-null int64\n",
      "num_journal           181057 non-null int64\n",
      "num_conference        181057 non-null int64\n",
      "degree                180847 non-null float64\n",
      "pagerank              180847 non-null float64\n",
      "degree_centrality     180847 non-null float64\n",
      "diff_year             180847 non-null float64\n",
      "coauthors_top10000    181057 non-null int64\n",
      "coauthors_top20000    181057 non-null int64\n",
      "coauthors_top30000    181057 non-null int64\n",
      "category              181057 non-null int64\n",
      "dtypes: float64(6), int64(14)\n",
      "memory usage: 27.6 MB\n",
      "data_test.info(): None\n"
     ]
    }
   ],
   "source": [
    "def get_predict_data(connection):\n",
    "    \"\"\"\n",
    "    获取需要预测的数据，包括训练集中的特征\n",
    "    :param connection:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    sql_select = \"\"\"\n",
    "    SELECT bys_cn, hindex_cn,a_conf+a_journal as a_paper, b_conf + b_journal as b_paper,c_conf + c_journal as c_paper,papernum2017, papernum2016, papernum2015, papernum2014, papernum2013,num_journal,num_conference, project_num, degree, pagerank,degree_centrality,last_year - first_year as diff_year , coauthors_top10000, coauthors_top20000, coauthors_top30000, category \n",
    "    FROM classifier_isTeacher_xgbc WHERE label is null and category is not null\n",
    "    \"\"\"\n",
    "    df = pd.read_sql_query(sql_select, connection)\n",
    "    all_features = ['bys_cn', 'hindex_cn', 'a_paper', 'b_paper', 'c_paper', 'papernum2017', 'papernum2016', 'papernum2015', 'papernum2014', 'papernum2013', 'num_journal', 'num_conference',  'degree', 'pagerank', 'degree_centrality', 'diff_year', 'coauthors_top10000', 'coauthors_top20000', 'coauthors_top30000', 'category']\n",
    "    data = df[all_features]\n",
    "    return data\n",
    "\n",
    "data_test = get_predict_data(connection)\n",
    "print(\"shape of data_test:\", data_test.shape)\n",
    "print(\"data_test.info():\", data_test.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4、处理predictData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 181057 entries, 0 to 181056\n",
      "Data columns (total 20 columns):\n",
      "bys_cn                181057 non-null float64\n",
      "hindex_cn             181057 non-null float64\n",
      "a_paper               181057 non-null int64\n",
      "b_paper               181057 non-null int64\n",
      "c_paper               181057 non-null int64\n",
      "papernum2017          181057 non-null int64\n",
      "papernum2016          181057 non-null int64\n",
      "papernum2015          181057 non-null int64\n",
      "papernum2014          181057 non-null int64\n",
      "papernum2013          181057 non-null int64\n",
      "num_journal           181057 non-null int64\n",
      "num_conference        181057 non-null int64\n",
      "degree                181057 non-null float64\n",
      "pagerank              181057 non-null float64\n",
      "degree_centrality     181057 non-null float64\n",
      "diff_year             181057 non-null float64\n",
      "coauthors_top10000    181057 non-null int64\n",
      "coauthors_top20000    181057 non-null int64\n",
      "coauthors_top30000    181057 non-null int64\n",
      "category              181057 non-null int64\n",
      "dtypes: float64(6), int64(14)\n",
      "memory usage: 27.6 MB\n",
      "info of data_test_fill:: None\n",
      "181057   20\n",
      "shape of X_test_all:: (181057, 22)\n"
     ]
    }
   ],
   "source": [
    "# 使用0进行填充\n",
    "data_test_fill = data_test.fillna(0)\n",
    "print(\"info of data_test_fill::\", data_test_fill.info())\n",
    "\n",
    "# 因为fDictVectorizer().fit_transform()需要的参数时list of dict，所以这里将其转化\n",
    "headers_test_all = list(data_test_fill.columns)\n",
    "value_df_train_all = data_test_fill.values\n",
    "feature_list_train_all = []\n",
    "for value_test_all in value_df_train_all:\n",
    "    feature_dict = {}\n",
    "    for i in range(0, len(headers_test_all)):\n",
    "        if headers_test_all[i]=='category':\n",
    "            feature_dict[headers_test_all[i]] = str(value_test_all[i])\n",
    "        else:\n",
    "            feature_dict[headers_test_all[i]] = value_test_all[i]\n",
    "    feature_list_train_all.append(feature_dict)\n",
    "print(len(feature_list_train_all), ' ', len(feature_list_train_all[0]))\n",
    "\n",
    "X_test_all = vec.transform(feature_list_train_all)\n",
    "\n",
    "print(\"shape of X_test_all::\", X_test_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5、使用SMOTE方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape (17539, 22)\n",
      "X_test.shape (833, 22)\n",
      "type of X_continuous_train_new:: <class 'scipy.sparse.csr.csr_matrix'>\n",
      "shape of X_continuous_train_new:: (17539, 19)\n",
      "shape of X_train:: (17539, 22)\n",
      "type of X_continuous_test_new:: <class 'scipy.sparse.csr.csr_matrix'>\n",
      "shape of X_continuous_test_new:: (833, 19)\n",
      "shape of X_test:: (833, 22)\n",
      "shape of X_train:: (17539, 22)\n",
      "shape of X_test:: (833, 22)\n",
      "shape of y_train:: (17539,)\n",
      "Counter of y_train:: Counter({1: 16551, 0: 988})\n",
      "shape of y_test:: (833,)\n",
      "Counter of y_test:: Counter({0: 683, 1: 150})\n",
      "shape of X_train_res:: (33102, 22)\n",
      "shape of y_train_res:: (33102,)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.86823   0.99164   0.92584       598\n",
      "           1    0.96667   0.61702   0.75325       235\n",
      "\n",
      "   micro avg    0.88595   0.88595   0.88595       833\n",
      "   macro avg    0.91745   0.80433   0.83954       833\n",
      "weighted avg    0.89600   0.88595   0.87715       833\n",
      "\n",
      "type of X_continuous_test_all:: <class 'scipy.sparse.csr.csr_matrix'>\n",
      "shape of X_continuous_test_all:: (181057, 19)\n",
      "shape::: (181057, 3)\n",
      "shape of X_test:: (181057, 22)\n",
      "y_predict:: Counter({0: 92635, 1: 88422})\n"
     ]
    }
   ],
   "source": [
    "def train_and_test_XGBC_SMOTE(X_train, y_train, X_test, y_test, X_test_all):\n",
    "    \n",
    "    print(\"X_train.shape\", X_train.shape)\n",
    "    print(\"X_test.shape\", X_test.shape)\n",
    "\n",
    "    # 归一化\n",
    "    ss = StandardScaler(with_mean=False)\n",
    "    X_continuous_train_new = ss.fit_transform(X_train[:, 0:-3])\n",
    "    print(\"type of X_continuous_train_new::\", type(X_continuous_train_new))\n",
    "    print(\"shape of X_continuous_train_new::\", X_continuous_train_new.shape)\n",
    "\n",
    "    # 将连续值和离散值拼接\n",
    "    X_train = np.hstack((X_continuous_train_new.A, X_train[:, -3:].A))   # .A转化为.numpy.ndarray\n",
    "    print(\"shape of X_train::\", X_train.shape)\n",
    "\n",
    "    # 归一化\n",
    "    X_continuous_test_new = ss.transform(X_test[:, 0:-3])\n",
    "    print(\"type of X_continuous_test_new::\", type(X_continuous_test_new))\n",
    "    print(\"shape of X_continuous_test_new::\", X_continuous_test_new.shape)\n",
    "\n",
    "    # 将连续值和离散值拼接\n",
    "    X_test = np.hstack((X_continuous_test_new.A, X_test[:, -3:].A))\n",
    "    print(\"shape of X_test::\", X_test.shape)\n",
    "\n",
    "    # 划分训练集和测试集\n",
    "    print(\"shape of X_train::\", X_train.shape)\n",
    "    print(\"shape of X_test::\", X_test.shape)\n",
    "    print(\"shape of y_train::\", y_train.shape)\n",
    "    print(\"Counter of y_train::\", Counter(y_train))\n",
    "    print(\"shape of y_test::\", y_test.shape)\n",
    "    print(\"Counter of y_test::\", Counter(y_test))\n",
    "    \n",
    "    sm = SMOTE(random_state=12, ratio=1.0)\n",
    "    X_train_res, y_train_res = sm.fit_sample(X_train, y_train)\n",
    "    print(\"shape of X_train_res::\", X_train_res.shape)\n",
    "    print(\"shape of y_train_res::\", y_train_res.shape)\n",
    "    \n",
    "    xgbc = XGBClassifier()\n",
    "    xgbc.fit(X_train_res, y_train_res)\n",
    "    y_test_predict = xgbc.predict(X_test)\n",
    "    print(classification_report(y_test_predict, y_test, target_names=['0', '1'], digits=5))\n",
    "\n",
    "    # 归一化\n",
    "    X_continuous_test_all = ss.transform(X_test_all[:, 0:-3])\n",
    "    print(\"type of X_continuous_test_all::\", type(X_continuous_test_all))\n",
    "    print(\"shape of X_continuous_test_all::\", X_continuous_test_all.shape)\n",
    "    \n",
    "    # 将连续值和离散值拼接\n",
    "    print(\"shape:::\", X_test_all[:, -3:].shape)\n",
    "    X_test_all = np.hstack((X_continuous_test_all.A, X_test_all[:, -3:].A))\n",
    "    print(\"shape of X_test::\", X_test_all.shape)\n",
    "    \n",
    "    y_predict = xgbc.predict(X_test_all)\n",
    "    print(\"y_predict::\", Counter(y_predict))\n",
    "    \n",
    "# 调用预测函数\n",
    "X_train_copy = deepcopy(X_train)    \n",
    "X_test_copy = deepcopy(X_test)\n",
    "y_train_copy = deepcopy(y_train)\n",
    "y_test_copy = deepcopy(y_test)\n",
    "train_and_test_XGBC_SMOTE(X_train_copy, y_train_copy, X_test_copy, y_test_copy, X_test_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6、使用ADASYN方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape (17539, 22)\n",
      "X_test.shape (833, 22)\n",
      "type of X_continuous_train_new:: <class 'scipy.sparse.csr.csr_matrix'>\n",
      "shape of X_continuous_train_new:: (17539, 19)\n",
      "shape of X_train:: (17539, 22)\n",
      "type of X_continuous_test_new:: <class 'scipy.sparse.csr.csr_matrix'>\n",
      "shape of X_continuous_test_new:: (833, 19)\n",
      "shape of X_test:: (833, 22)\n",
      "shape of X_train:: (17539, 22)\n",
      "shape of X_test:: (833, 22)\n",
      "shape of y_train:: (17539,)\n",
      "Counter of y_train:: Counter({1: 16551, 0: 988})\n",
      "shape of y_test:: (833,)\n",
      "Counter of y_test:: Counter({0: 683, 1: 150})\n",
      "shape of X_train_res:: (33028, 22)\n",
      "shape of y_train_res:: (33028,)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.90776   0.98569   0.94512       629\n",
      "           1    0.94000   0.69118   0.79661       204\n",
      "\n",
      "   micro avg    0.91357   0.91357   0.91357       833\n",
      "   macro avg    0.92388   0.83843   0.87087       833\n",
      "weighted avg    0.91566   0.91357   0.90875       833\n",
      "\n",
      "type of X_continuous_test_all:: <class 'scipy.sparse.csr.csr_matrix'>\n",
      "shape of X_continuous_test_all:: (181057, 19)\n",
      "shape::: (181057, 3)\n",
      "shape of X_test:: (181057, 22)\n",
      "y_predict:: Counter({0: 98513, 1: 82544})\n"
     ]
    }
   ],
   "source": [
    "def train_and_test_XGBC_ADASYN(X_train, y_train, X_test, y_test, X_test_all):\n",
    "    \n",
    "    print(\"X_train.shape\", X_train.shape)\n",
    "    print(\"X_test.shape\", X_test.shape)\n",
    "\n",
    "    # 归一化\n",
    "    ss = StandardScaler(with_mean=False)\n",
    "    X_continuous_train_new = ss.fit_transform(X_train[:, 0:-3])\n",
    "    print(\"type of X_continuous_train_new::\", type(X_continuous_train_new))\n",
    "    print(\"shape of X_continuous_train_new::\", X_continuous_train_new.shape)\n",
    "\n",
    "    # 将连续值和离散值拼接\n",
    "    X_train = np.hstack((X_continuous_train_new.A, X_train[:, -3:].A))   # .A转化为.numpy.ndarray\n",
    "    print(\"shape of X_train::\", X_train.shape)\n",
    "\n",
    "    # 归一化\n",
    "    X_continuous_test_new = ss.transform(X_test[:, 0:-3])\n",
    "    print(\"type of X_continuous_test_new::\", type(X_continuous_test_new))\n",
    "    print(\"shape of X_continuous_test_new::\", X_continuous_test_new.shape)\n",
    "\n",
    "    # 将连续值和离散值拼接\n",
    "    X_test = np.hstack((X_continuous_test_new.A, X_test[:, -3:].A))\n",
    "    print(\"shape of X_test::\", X_test.shape)\n",
    "\n",
    "    # 划分训练集和测试集\n",
    "    print(\"shape of X_train::\", X_train.shape)\n",
    "    print(\"shape of X_test::\", X_test.shape)\n",
    "    print(\"shape of y_train::\", y_train.shape)\n",
    "    print(\"Counter of y_train::\", Counter(y_train))\n",
    "    print(\"shape of y_test::\", y_test.shape)\n",
    "    print(\"Counter of y_test::\", Counter(y_test))\n",
    "    \n",
    "    sm = ADASYN(random_state=12, ratio=1.0)\n",
    "    X_train_res, y_train_res = sm.fit_sample(X_train, y_train)\n",
    "    print(\"shape of X_train_res::\", X_train_res.shape)\n",
    "    print(\"shape of y_train_res::\", y_train_res.shape)\n",
    "    \n",
    "    xgbc = XGBClassifier()\n",
    "    xgbc.fit(X_train_res, y_train_res)\n",
    "    y_test_predict = xgbc.predict(X_test)\n",
    "    print(classification_report(y_test_predict, y_test, target_names=['0', '1'], digits=5))\n",
    "\n",
    "    # 归一化\n",
    "    X_continuous_test_all = ss.transform(X_test_all[:, 0:-3])\n",
    "    print(\"type of X_continuous_test_all::\", type(X_continuous_test_all))\n",
    "    print(\"shape of X_continuous_test_all::\", X_continuous_test_all.shape)\n",
    "    \n",
    "    # 将连续值和离散值拼接\n",
    "    print(\"shape:::\", X_test_all[:, -3:].shape)\n",
    "    X_test_all = np.hstack((X_continuous_test_all.A, X_test_all[:, -3:].A))\n",
    "    print(\"shape of X_test::\", X_test_all.shape)\n",
    "    \n",
    "    y_predict = xgbc.predict(X_test_all)\n",
    "    print(\"y_predict::\", Counter(y_predict))\n",
    "    \n",
    "# 调用预测函数\n",
    "X_train_copy = deepcopy(X_train)    \n",
    "X_test_copy = deepcopy(X_test)\n",
    "y_train_copy = deepcopy(y_train)\n",
    "y_test_copy = deepcopy(y_test)\n",
    "train_and_test_XGBC_ADASYN(X_train_copy, y_train_copy, X_test_copy, y_test_copy, X_test_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7、使用随机上采样方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape (17539, 22)\n",
      "X_test.shape (833, 22)\n",
      "type of X_continuous_train_new:: <class 'scipy.sparse.csr.csr_matrix'>\n",
      "shape of X_continuous_train_new:: (17539, 19)\n",
      "shape of X_train:: (17539, 22)\n",
      "type of X_continuous_test_new:: <class 'scipy.sparse.csr.csr_matrix'>\n",
      "shape of X_continuous_test_new:: (833, 19)\n",
      "shape of X_test:: (833, 22)\n",
      "shape of X_train:: (17539, 22)\n",
      "shape of X_test:: (833, 22)\n",
      "shape of y_train:: (17539,)\n",
      "Counter of y_train:: Counter({1: 16551, 0: 988})\n",
      "shape of y_test:: (833,)\n",
      "Counter of y_test:: Counter({0: 683, 1: 150})\n",
      "shape of X_train_res:: (33102, 22)\n",
      "shape of y_train_res:: (33102,)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.95461   0.98341   0.96880       663\n",
      "           1    0.92667   0.81765   0.86875       170\n",
      "\n",
      "   micro avg    0.94958   0.94958   0.94958       833\n",
      "   macro avg    0.94064   0.90053   0.91877       833\n",
      "weighted avg    0.94891   0.94958   0.94838       833\n",
      "\n",
      "type of X_continuous_test_all:: <class 'scipy.sparse.csr.csr_matrix'>\n",
      "shape of X_continuous_test_all:: (181057, 19)\n",
      "shape::: (181057, 3)\n",
      "shape of X_test:: (181057, 22)\n",
      "y_predict:: Counter({0: 108887, 1: 72170})\n"
     ]
    }
   ],
   "source": [
    "def train_and_test_XGBC_ROS(X_train, y_train, X_test, y_test, X_test_all):\n",
    "    \n",
    "    print(\"X_train.shape\", X_train.shape)\n",
    "    print(\"X_test.shape\", X_test.shape)\n",
    "\n",
    "    # 归一化\n",
    "    ss = StandardScaler(with_mean=False)\n",
    "    X_continuous_train_new = ss.fit_transform(X_train[:, 0:-3])\n",
    "    print(\"type of X_continuous_train_new::\", type(X_continuous_train_new))\n",
    "    print(\"shape of X_continuous_train_new::\", X_continuous_train_new.shape)\n",
    "\n",
    "    # 将连续值和离散值拼接\n",
    "    X_train = np.hstack((X_continuous_train_new.A, X_train[:, -3:].A))   # .A转化为.numpy.ndarray\n",
    "    print(\"shape of X_train::\", X_train.shape)\n",
    "\n",
    "    # 归一化\n",
    "    X_continuous_test_new = ss.transform(X_test[:, 0:-3])\n",
    "    print(\"type of X_continuous_test_new::\", type(X_continuous_test_new))\n",
    "    print(\"shape of X_continuous_test_new::\", X_continuous_test_new.shape)\n",
    "\n",
    "    # 将连续值和离散值拼接\n",
    "    X_test = np.hstack((X_continuous_test_new.A, X_test[:, -3:].A))\n",
    "    print(\"shape of X_test::\", X_test.shape)\n",
    "\n",
    "    # 划分训练集和测试集\n",
    "    print(\"shape of X_train::\", X_train.shape)\n",
    "    print(\"shape of X_test::\", X_test.shape)\n",
    "    print(\"shape of y_train::\", y_train.shape)\n",
    "    print(\"Counter of y_train::\", Counter(y_train))\n",
    "    print(\"shape of y_test::\", y_test.shape)\n",
    "    print(\"Counter of y_test::\", Counter(y_test))\n",
    "    \n",
    "    sm = RandomOverSampler(random_state=12, ratio=1.0)\n",
    "    X_train_res, y_train_res = sm.fit_sample(X_train, y_train)\n",
    "    print(\"shape of X_train_res::\", X_train_res.shape)\n",
    "    print(\"shape of y_train_res::\", y_train_res.shape)\n",
    "    \n",
    "    xgbc = XGBClassifier()\n",
    "    xgbc.fit(X_train_res, y_train_res)\n",
    "    y_test_predict = xgbc.predict(X_test)\n",
    "    print(classification_report(y_test_predict, y_test, target_names=['0', '1'], digits=5))\n",
    "\n",
    "    # 归一化\n",
    "    X_continuous_test_all = ss.transform(X_test_all[:, 0:-3])\n",
    "    print(\"type of X_continuous_test_all::\", type(X_continuous_test_all))\n",
    "    print(\"shape of X_continuous_test_all::\", X_continuous_test_all.shape)\n",
    "    \n",
    "    # 将连续值和离散值拼接\n",
    "    print(\"shape:::\", X_test_all[:, -3:].shape)\n",
    "    X_test_all = np.hstack((X_continuous_test_all.A, X_test_all[:, -3:].A))\n",
    "    print(\"shape of X_test::\", X_test_all.shape)\n",
    "    \n",
    "    y_predict = xgbc.predict(X_test_all)\n",
    "    print(\"y_predict::\", Counter(y_predict))\n",
    "    \n",
    "# 调用预测函数\n",
    "X_train_copy = deepcopy(X_train)    \n",
    "X_test_copy = deepcopy(X_test)\n",
    "y_train_copy = deepcopy(y_train)\n",
    "y_test_copy = deepcopy(y_test)\n",
    "train_and_test_XGBC_ROS(X_train_copy, y_train_copy, X_test_copy, y_test_copy, X_test_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "从上面我们可以知道，使用随机上采用的结果比较好，这个结果和使用pu-learning的相当。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
