{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 功能简介\n",
    "\n",
    "使用pu-learning算法解决样本负例非常少的情况。\n",
    "\n",
    "pu-learning主要有三种思路，这里使用pu-bagging和two-step的方法，介绍详解参考文章或者博客：\n",
    "\n",
    "参考文章：https://roywright.me/2017/11/16/positive-unlabeled-learning/\n",
    "\n",
    "引用的baggingPU.py来自：https://github.com/roywright/pu_learning/blob/master/baggingPU.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.feature_extraction import DictVectorizer  \n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "from xgboost import XGBClassifier\n",
    "from baggingPU import BaggingClassifierPU\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from math import isnan\n",
    "from numpy import NaN\n",
    "from numpy import nan\n",
    "import pickle\n",
    "import json\n",
    "from collections import Counter\n",
    "from copy import deepcopy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1、获取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = pymysql.Connect(\n",
    "    host=\"localhost\",\n",
    "    port=3306,\n",
    "    user=\"root\",\n",
    "    passwd=\"root\",\n",
    "    charset=\"utf8\",\n",
    "    db=\"project_researchers\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train_data: (198901, 21)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 198901 entries, 0 to 198900\n",
      "Data columns (total 21 columns):\n",
      "bys_cn                194419 non-null float64\n",
      "hindex_cn             198331 non-null float64\n",
      "a_paper               198901 non-null int64\n",
      "b_paper               198901 non-null int64\n",
      "c_paper               198901 non-null int64\n",
      "papernum2017          198901 non-null int64\n",
      "papernum2016          198901 non-null int64\n",
      "papernum2015          198901 non-null int64\n",
      "papernum2014          198901 non-null int64\n",
      "papernum2013          198901 non-null int64\n",
      "num_journal           198901 non-null int64\n",
      "num_conference        198901 non-null int64\n",
      "degree                198621 non-null float64\n",
      "pagerank              198621 non-null float64\n",
      "degree_centrality     198621 non-null float64\n",
      "diff_year             198621 non-null float64\n",
      "coauthors_top10000    198901 non-null int64\n",
      "coauthors_top20000    198901 non-null int64\n",
      "coauthors_top30000    198901 non-null int64\n",
      "category              198901 non-null int64\n",
      "label                 198901 non-null int64\n",
      "dtypes: float64(6), int64(15)\n",
      "memory usage: 31.9 MB\n",
      "train_data.info(): None\n"
     ]
    }
   ],
   "source": [
    "def get_train_data(connection):\n",
    "    \"\"\"\n",
    "    查询数据，包括特征和标签\n",
    "    :param connection:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    sql_select = \"\"\"\n",
    "     SELECT bys_cn, hindex_cn,a_conf+a_journal as a_paper, b_conf + b_journal as b_paper,c_conf + c_journal as c_paper,papernum2017, papernum2016, papernum2015, papernum2014, papernum2013,num_journal,num_conference, project_num, degree, pagerank,degree_centrality,last_year - first_year as diff_year , coauthors_top10000, coauthors_top20000, coauthors_top30000, category, label \n",
    "     FROM classifier_isTeacher_xgbc WHERE label = 1 and teac_id > 174 and category is not null\n",
    "     UNION ALL\n",
    "     SELECT bys_cn, hindex_cn,a_conf+a_journal as a_paper, b_conf + b_journal as b_paper,c_conf + c_journal as c_paper,papernum2017, papernum2016, papernum2015, papernum2014, papernum2013,num_journal,num_conference, project_num, degree, pagerank,degree_centrality,last_year - first_year as diff_year , coauthors_top10000, coauthors_top20000, coauthors_top30000, category, label \n",
    "     FROM classifier_isTeacher_xgbc WHERE label = 0 and teac_id > 64438 and category is not null\n",
    "     UNION ALL \n",
    "     SELECT bys_cn, hindex_cn,a_conf+a_journal as a_paper, b_conf + b_journal as b_paper,c_conf + c_journal as c_paper,papernum2017, papernum2016, papernum2015, papernum2014, papernum2013,num_journal,num_conference, project_num, degree, pagerank,degree_centrality,last_year - first_year as diff_year , coauthors_top10000, coauthors_top20000, coauthors_top30000, category, 0\n",
    "     FROM classifier_isTeacher_xgbc WHERE label is null and category is not null\n",
    "    \"\"\"\n",
    "    df = pd.read_sql_query(sql_select, connection)\n",
    "    all_features = ['bys_cn', 'hindex_cn', 'a_paper', 'b_paper', 'c_paper', 'papernum2017', 'papernum2016', 'papernum2015', 'papernum2014', 'papernum2013', 'num_journal', 'num_conference',  'degree', 'pagerank', 'degree_centrality', 'diff_year', 'coauthors_top10000', 'coauthors_top20000', 'coauthors_top30000', 'category', 'label']\n",
    "    data = df[all_features]\n",
    "    return data\n",
    "\n",
    "train_data = get_train_data(connection)\n",
    "print(\"shape of train_data:\", train_data.shape)\n",
    "print(\"train_data.info():\", train_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of test_data: (850, 21)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 850 entries, 0 to 849\n",
      "Data columns (total 21 columns):\n",
      "bys_cn                834 non-null float64\n",
      "hindex_cn             850 non-null int64\n",
      "a_paper               850 non-null int64\n",
      "b_paper               850 non-null int64\n",
      "c_paper               850 non-null int64\n",
      "papernum2017          850 non-null int64\n",
      "papernum2016          850 non-null int64\n",
      "papernum2015          850 non-null int64\n",
      "papernum2014          850 non-null int64\n",
      "papernum2013          850 non-null int64\n",
      "num_journal           850 non-null int64\n",
      "num_conference        850 non-null int64\n",
      "degree                849 non-null float64\n",
      "pagerank              849 non-null float64\n",
      "degree_centrality     849 non-null float64\n",
      "diff_year             849 non-null float64\n",
      "coauthors_top10000    850 non-null int64\n",
      "coauthors_top20000    850 non-null int64\n",
      "coauthors_top30000    850 non-null int64\n",
      "category              850 non-null int64\n",
      "label                 850 non-null int64\n",
      "dtypes: float64(5), int64(16)\n",
      "memory usage: 139.5 KB\n",
      "test_data.info(): None\n"
     ]
    }
   ],
   "source": [
    "def get_test_data(connection):\n",
    "    \"\"\"\n",
    "    查询测试数据，包括特征和标签\n",
    "    :param connection:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    sql_select = \"\"\"\n",
    "     SELECT bys_cn, hindex_cn,a_conf+a_journal as a_paper, b_conf + b_journal as b_paper,c_conf + c_journal as c_paper,papernum2017, papernum2016, papernum2015, papernum2014, papernum2013,num_journal,num_conference, project_num, degree, pagerank,degree_centrality,last_year - first_year as diff_year , coauthors_top10000, coauthors_top20000, coauthors_top30000, category, label \n",
    "     FROM classifier_isTeacher_xgbc WHERE label = 1 and teac_id <= 174 and category is not null\n",
    "     UNION ALL\n",
    "     SELECT bys_cn, hindex_cn,a_conf+a_journal as a_paper, b_conf + b_journal as b_paper,c_conf + c_journal as c_paper,papernum2017, papernum2016, papernum2015, papernum2014, papernum2013,num_journal,num_conference, project_num, degree, pagerank,degree_centrality,last_year - first_year as diff_year , coauthors_top10000, coauthors_top20000, coauthors_top30000, category, label \n",
    "     FROM classifier_isTeacher_xgbc WHERE label = 0 and teac_id <= 64438 and category is not null\n",
    "    \"\"\"\n",
    "    df = pd.read_sql_query(sql_select, connection)\n",
    "    all_features = ['bys_cn', 'hindex_cn', 'a_paper', 'b_paper', 'c_paper', 'papernum2017', 'papernum2016', 'papernum2015', 'papernum2014', 'papernum2013', 'num_journal', 'num_conference',  'degree', 'pagerank', 'degree_centrality', 'diff_year', 'coauthors_top10000', 'coauthors_top20000', 'coauthors_top30000', 'category', 'label']\n",
    "    data = df[all_features]\n",
    "    return data\n",
    "\n",
    "test_data = get_test_data(connection)\n",
    "print(\"shape of test_data:\", test_data.shape)\n",
    "print(\"test_data.info():\", test_data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 198901 entries, 0 to 198900\n",
      "Data columns (total 21 columns):\n",
      "bys_cn                198901 non-null float64\n",
      "hindex_cn             198901 non-null float64\n",
      "a_paper               198901 non-null int64\n",
      "b_paper               198901 non-null int64\n",
      "c_paper               198901 non-null int64\n",
      "papernum2017          198901 non-null int64\n",
      "papernum2016          198901 non-null int64\n",
      "papernum2015          198901 non-null int64\n",
      "papernum2014          198901 non-null int64\n",
      "papernum2013          198901 non-null int64\n",
      "num_journal           198901 non-null int64\n",
      "num_conference        198901 non-null int64\n",
      "degree                198901 non-null float64\n",
      "pagerank              198901 non-null float64\n",
      "degree_centrality     198901 non-null float64\n",
      "diff_year             198901 non-null float64\n",
      "coauthors_top10000    198901 non-null int64\n",
      "coauthors_top20000    198901 non-null int64\n",
      "coauthors_top30000    198901 non-null int64\n",
      "category              198901 non-null int64\n",
      "label                 198901 non-null int64\n",
      "dtypes: float64(6), int64(15)\n",
      "memory usage: 31.9 MB\n",
      "info of train_data:: None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 850 entries, 0 to 849\n",
      "Data columns (total 21 columns):\n",
      "bys_cn                850 non-null float64\n",
      "hindex_cn             850 non-null int64\n",
      "a_paper               850 non-null int64\n",
      "b_paper               850 non-null int64\n",
      "c_paper               850 non-null int64\n",
      "papernum2017          850 non-null int64\n",
      "papernum2016          850 non-null int64\n",
      "papernum2015          850 non-null int64\n",
      "papernum2014          850 non-null int64\n",
      "papernum2013          850 non-null int64\n",
      "num_journal           850 non-null int64\n",
      "num_conference        850 non-null int64\n",
      "degree                850 non-null float64\n",
      "pagerank              850 non-null float64\n",
      "degree_centrality     850 non-null float64\n",
      "diff_year             850 non-null float64\n",
      "coauthors_top10000    850 non-null int64\n",
      "coauthors_top20000    850 non-null int64\n",
      "coauthors_top30000    850 non-null int64\n",
      "category              850 non-null int64\n",
      "label                 850 non-null int64\n",
      "dtypes: float64(5), int64(16)\n",
      "memory usage: 139.5 KB\n",
      "info of test_data:: None\n"
     ]
    }
   ],
   "source": [
    "# 对缺失值进行处理\n",
    "# Method1：直接将含有缺失字段的值去掉\n",
    "columns_name_zero = ['bys_cn', 'hindex_cn', 'degree', 'pagerank', 'degree_centrality', 'diff_year']\n",
    "for column_name in columns_name_zero:\n",
    "    train_data[column_name].fillna(0, inplace=True)\n",
    "    test_data[column_name].fillna(0, inplace=True)\n",
    "print(\"info of train_data::\", train_data.info())\n",
    "print(\"info of test_data::\", test_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** (198901, 20)\n",
      "198901   20\n",
      "info of X_train: (198901, 22)\n",
      "y_train:: Counter({0: 182066, 1: 16835})\n",
      "850   20\n",
      "info of X_test: (850, 22)\n",
      "y_test:: Counter({0: 700, 1: 150})\n"
     ]
    }
   ],
   "source": [
    "# 将category变成离散值（object），并将train_data变成向量（离散值变成one-hot == 使用DictVectorizer）\n",
    "\n",
    "# training data\n",
    "train_data[['category']] = train_data[['category']].astype(object)\n",
    "y_train = train_data['label']\n",
    "X_train = train_data.drop(columns=['label'])\n",
    "print(\"****\", X_train.shape)\n",
    "\n",
    "vec = DictVectorizer()   \n",
    "\n",
    "# 因为fDictVectorizer().fit_transform()需要的参数时list of dict，所以这里将其转化\n",
    "headers_train = list(X_train.columns)\n",
    "value_df_train = X_train.values\n",
    "feature_list_train = []\n",
    "for value_train in value_df_train:\n",
    "    feature_dict = {}\n",
    "    for i in range(0, len(headers_train)):\n",
    "        if headers_train[i]=='category':\n",
    "            feature_dict[headers_train[i]] = str(value_train[i])\n",
    "        else:\n",
    "            feature_dict[headers_train[i]] = value_train[i]\n",
    "    feature_list_train.append(feature_dict)\n",
    "print(len(feature_list_train), ' ', len(feature_list_train[0]))\n",
    "\n",
    "X_train = vec.fit_transform(feature_list_train)\n",
    "print(\"info of X_train:\", X_train.shape)\n",
    "print(\"y_train::\", Counter(y_train))\n",
    "\n",
    "# testing data\n",
    "test_data[['category']] = test_data[['category']].astype(object)\n",
    "y_test = test_data['label']\n",
    "X_test = test_data.drop(columns=['label'])\n",
    "\n",
    "headers_test = list(X_test.columns)\n",
    "value_df_test = X_test.values\n",
    "# print(headers)\n",
    "# print(value_df.shape)\n",
    "feature_list_test = []\n",
    "for value_test in value_df_test:\n",
    "    feature_dict = {}\n",
    "    for i in range(0, len(headers_test)):\n",
    "        if headers_test[i]=='category':\n",
    "            feature_dict[headers_test[i]] = str(value_test[i])\n",
    "        else:\n",
    "            feature_dict[headers_test[i]] = value_test[i]\n",
    "    feature_list_test.append(feature_dict)\n",
    "print(len(feature_list_test), ' ', len(feature_list_test[0]))\n",
    "\n",
    "X_test = vec.transform(feature_list_test)\n",
    "print(\"info of X_test:\", X_test.shape)\n",
    "print(\"y_test::\", Counter(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3、PU-Learning\n",
    "\n",
    "### 3.1 pu-bagging方法\n",
    "\n",
    "pu-bagging借助了bagging的思想，步骤如下：\n",
    "\n",
    "（1）采样与正例相同大小的无标签数据当做负样本\n",
    "\n",
    "（2）使用正例和负例训练分类器，预测除此正例和负例之外的数据标签\n",
    "\n",
    "（3）重复多次，取预测的平均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of X_continuous_train_new:: <class 'scipy.sparse.csr.csr_matrix'>\n",
      "shape of X_continuous_train_new:: (198901, 19)\n",
      "shape of X_train:: (198901, 22)\n",
      "type of X_continuous_test_new:: <class 'scipy.sparse.csr.csr_matrix'>\n",
      "shape of X_continuous_test_new:: (850, 19)\n",
      "shape of X_test:: (850, 22)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.95714   0.97953   0.96821       684\n",
      "           1    0.90667   0.81928   0.86076       166\n",
      "\n",
      "   micro avg    0.94824   0.94824   0.94824       850\n",
      "   macro avg    0.93190   0.89940   0.91448       850\n",
      "weighted avg    0.94729   0.94824   0.94722       850\n",
      "\n",
      "Counter({0: 147926, 1: 50975})\n"
     ]
    }
   ],
   "source": [
    "def train_and_test_XGBC_puBagging(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    # 归一化\n",
    "    ss = StandardScaler(with_mean=False)\n",
    "    X_continuous_train_new = ss.fit_transform(X_train[:, 0:-3])\n",
    "    print(\"type of X_continuous_train_new::\", type(X_continuous_train_new))\n",
    "    print(\"shape of X_continuous_train_new::\", X_continuous_train_new.shape)\n",
    "\n",
    "    # 将连续值和离散值拼接\n",
    "    X_train = np.hstack((X_continuous_train_new.A, X_train[:, -3:].A))   # .A转化为.numpy.ndarray\n",
    "    print(\"shape of X_train::\", X_train.shape)\n",
    "\n",
    "    # 归一化\n",
    "    X_continuous_test_new = ss.transform(X_test[:, 0:-3])\n",
    "    print(\"type of X_continuous_test_new::\", type(X_continuous_test_new))\n",
    "    print(\"shape of X_continuous_test_new::\", X_continuous_test_new.shape)\n",
    "\n",
    "    # 将连续值和离散值拼接\n",
    "    X_test = np.hstack((X_continuous_test_new.A, X_test[:, -3:].A))\n",
    "    print(\"shape of X_test::\", X_test.shape)\n",
    "    \n",
    "    y_train_origin = y_train.copy()\n",
    "    \n",
    "#     MAX_SAMPLES = 2 * np.array(y_train).tolist().count(1)\n",
    "#     print(MAX_SAMPLES)\n",
    "\n",
    "    bc = BaggingClassifierPU(\n",
    "        DecisionTreeClassifier(),\n",
    "        n_estimators=300,  # 1000 trees as usual\n",
    "        max_samples=sum(y_train),  # Balance the positives and unlabeled in each bag\n",
    "    )\n",
    "    bc.fit(X_train, y_train)\n",
    "    \n",
    "    y_test_predict = bc.predict(X_test)\n",
    "    \n",
    "    print(classification_report(y_test_predict, y_test, target_names=['0', '1'], digits=5))\n",
    "    \n",
    "    # 统计总共的数据预测出来多少教师\n",
    "    y_all_predict = bc.predict(X_train)\n",
    "    print(Counter(y_all_predict))\n",
    "\n",
    "\n",
    "# 调用预测函数\n",
    "X_train_copy = deepcopy(X_train)\n",
    "y_train_copy = deepcopy(y_train)\n",
    "X_test_copy = deepcopy(X_test)\n",
    "y_test_copy = deepcopy(y_test)\n",
    "train_and_test_XGBC_puBagging(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Two-step\n",
    "\n",
    "two-step的思想如下：\n",
    "\n",
    "（1）首先将所有的无标签数据当做负样本，和所有正例当做训练集训练分类器，识别出无标签样本数据中可靠的负例，将其当做真正的负例。\n",
    "\n",
    "（2）使用正例和Step1中的可靠负例训练分类器，在挑选中可靠负例，不断迭代（本次实验迭代了10次）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of X_continuous_train_new:: <class 'scipy.sparse.csr.csr_matrix'>\n",
      "shape of X_continuous_train_new:: (198901, 19)\n",
      "shape of X_train:: (198901, 22)\n",
      "type of X_continuous_test_new:: <class 'scipy.sparse.csr.csr_matrix'>\n",
      "shape of X_continuous_test_new:: (850, 19)\n",
      "shape of X_test:: (850, 22)\n",
      "pred is:: [0.7 0.8 0.8 ... 0.3 0.  0. ]\n",
      "Step 1 labeled 0 new positives and 143341 new negatives.\n",
      "Doing step1... range_P is:: [0.0, 0.9]\n",
      "Step 1 labeled 1 new positives and 15622 new negatives.\n",
      "Doing step2... range_P is:: [0.0, 0.9]\n",
      "Step 1 labeled 0 new positives and 5528 new negatives.\n",
      "Doing step3... range_P is:: [0.0, 0.9]\n",
      "Step 1 labeled 0 new positives and 2774 new negatives.\n",
      "Doing step4... range_P is:: [0.0, 0.9]\n",
      "Step 1 labeled 0 new positives and 1493 new negatives.\n",
      "Doing step5... range_P is:: [0.0, 0.9]\n",
      "Step 1 labeled 0 new positives and 969 new negatives.\n",
      "Doing step6... range_P is:: [0.0, 0.9]\n",
      "Step 1 labeled 0 new positives and 663 new negatives.\n",
      "Doing step7... range_P is:: [0.0, 0.9]\n",
      "Step 1 labeled 0 new positives and 454 new negatives.\n",
      "Doing step8... range_P is:: [0.0, 0.9]\n",
      "Step 1 labeled 0 new positives and 354 new negatives.\n",
      "Doing step9... range_P is:: [0.0, 0.9]\n",
      "Step 1 labeled 0 new positives and 280 new negatives.\n",
      "Doing step10... range_P is:: [0.0, 0.9]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   0.89629   0.94531       781\n",
      "           1    0.46000   1.00000   0.63014        69\n",
      "\n",
      "   micro avg    0.90471   0.90471   0.90471       850\n",
      "   macro avg    0.73000   0.94814   0.78772       850\n",
      "weighted avg    0.95616   0.90471   0.91972       850\n",
      "\n",
      "Counter({0: 172908, 1: 15185, -1: 10808})\n"
     ]
    }
   ],
   "source": [
    "def train_and_test_XGBC_puTwoStep(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    # 归一化\n",
    "    ss = StandardScaler(with_mean=False)\n",
    "    X_continuous_train_new = ss.fit_transform(X_train[:, 0:-3])\n",
    "    print(\"type of X_continuous_train_new::\", type(X_continuous_train_new))\n",
    "    print(\"shape of X_continuous_train_new::\", X_continuous_train_new.shape)\n",
    "\n",
    "    # 将连续值和离散值拼接\n",
    "    X_train = np.hstack((X_continuous_train_new.A, X_train[:, -3:].A))   # .A转化为.numpy.ndarray\n",
    "    print(\"shape of X_train::\", X_train.shape)\n",
    "\n",
    "    # 归一化\n",
    "    X_continuous_test_new = ss.transform(X_test[:, 0:-3])\n",
    "    print(\"type of X_continuous_test_new::\", type(X_continuous_test_new))\n",
    "    print(\"shape of X_continuous_test_new::\", X_continuous_test_new.shape)\n",
    "\n",
    "    # 将连续值和离散值拼接\n",
    "    X_test = np.hstack((X_continuous_test_new.A, X_test[:, -3:].A))\n",
    "    print(\"shape of X_test::\", X_test.shape)\n",
    "    \n",
    "    # Create a new target vector, with 1 for positive, -1 for unlabeled, and \n",
    "    # 0 for \"reliable negative\" (there are no reliable negatives to start with)\n",
    "    y_train_c = 2*y_train - 1 \n",
    "    \n",
    "    # Get the scores from RandomForestClassifier\n",
    "    rf = RandomForestClassifier(n_estimators = 10)   # Use 1000 trees\n",
    "    rf.fit(X_train, y_train_c)\n",
    "    pred = rf.predict_proba(X_train)[:,1]\n",
    "    print(\"pred is::\", pred)\n",
    "    \n",
    "    # Find the range of scores given to positive data points\n",
    "    # range_P = [min(pred * (y_train_c > 0)), max(pred * (y_train_c > 0))]\n",
    "    range_P = [0.0, 0.9]\n",
    "\n",
    "    # STEP 1\n",
    "    # If any unlabeled point has a score above all known positives, \n",
    "    # or below all known positives, label it accordingly\n",
    "    iP_new = y_train_c[(y_train_c < 0) & (pred >= range_P[1])].index\n",
    "    iN_new = y_train_c[(y_train_c < 0) & (pred <= range_P[0])].index\n",
    "    y_train_c.loc[iP_new] = 1\n",
    "    y_train_c.loc[iN_new] = 0\n",
    "    \n",
    "    \n",
    "    # Classifier to be used for step 2\n",
    "    rf2 = RandomForestClassifier(n_estimators = 10)\n",
    "\n",
    "    # Limit to 10 iterations (this is arbitrary, but \n",
    "    # otherwise this approach can take a very long time)\n",
    "    for i in range(10):\n",
    "        # If step 1 didn't find new labels, we're done\n",
    "        if len(iP_new) + len(iN_new) == 0 and i > 0:\n",
    "            break\n",
    "\n",
    "        print('Step 1 labeled %d new positives and %d new negatives.' % (len(iP_new), len(iN_new)))\n",
    "        print('Doing step' + str(i+1) + '... ', end = '')\n",
    "\n",
    "        # STEP 2\n",
    "        # Retrain on new labels and get new scores\n",
    "        rf2.fit(X_train, y_train_c)\n",
    "        pred = rf2.predict_proba(X_train)[:,-1]\n",
    "\n",
    "        # Find the range of scores given to positive data points\n",
    "        # range_P = [min(pred * (y_train_c > 0)), max(pred * (y_train_c > 0))]\n",
    "        range_P = [0.0, 0.9]\n",
    "        print(\"range_P is::\", range_P)\n",
    "\n",
    "        # Repeat step 1\n",
    "        iP_new = y_train_c[(y_train_c < 0) & (pred >= range_P[1])].index\n",
    "        iN_new = y_train_c[(y_train_c < 0) & (pred <= range_P[0])].index\n",
    "        y_train_c.loc[iP_new] = 1\n",
    "        y_train_c.loc[iN_new] = 0\n",
    "        \n",
    "    y_test_predict = rf2.predict(X_test)\n",
    "    for i in range(len(y_test_predict)):\n",
    "        if y_test_predict[i] == -1:\n",
    "            y_test_predict[i] = 0\n",
    "    print(classification_report(y_test_predict, y_test, target_names=[ '0', '1'], digits=5))\n",
    "    \n",
    "    # 统计总共的数据预测出来多少教师\n",
    "    y_all_predict = rf2.predict(X_train)\n",
    "    print(Counter(y_all_predict))\n",
    "\n",
    "# 调用预测函数\n",
    "X_train_copy = deepcopy(X_train)\n",
    "y_train_copy = deepcopy(y_train)\n",
    "X_test_copy = deepcopy(X_test)\n",
    "y_test_copy = deepcopy(y_test)\n",
    "train_and_test_XGBC_puTwoStep(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分析\n",
    "\n",
    "通过上述结果发现，对于本任务而言，使用pu-bagging的效果要好于使用two-step的效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
