{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 功能简介\n",
    "\n",
    "使用pu-learning算法解决样本负例非常少的情况。\n",
    "\n",
    "pu-learning主要有三种思路，这里使用pu-bagging和two-step的方法，介绍详解参考文章或者博客：\n",
    "\n",
    "参考文章：https://roywright.me/2017/11/16/positive-unlabeled-learning/\n",
    "\n",
    "引用的baggingPU.py来自：https://github.com/roywright/pu_learning/blob/master/baggingPU.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymysql\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "from xgboost import XGBClassifier\n",
    "from baggingPU import BaggingClassifierPU\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from math import isnan\n",
    "from numpy import NaN\n",
    "from numpy import nan\n",
    "import pickle\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1、获取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "connection = pymysql.Connect(\n",
    "    host=\"localhost\",\n",
    "    port=3306,\n",
    "    user=\"root\",\n",
    "    passwd=\"root\",\n",
    "    charset=\"utf8\",\n",
    "    db=\"project_researchers\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of data: (199751, 22)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 199751 entries, 0 to 199750\n",
      "Data columns (total 22 columns):\n",
      "teac_id               199751 non-null int64\n",
      "bys_cn                195253 non-null float64\n",
      "hindex_cn             199181 non-null float64\n",
      "a_paper               199751 non-null int64\n",
      "b_paper               199751 non-null int64\n",
      "c_paper               199751 non-null int64\n",
      "papernum2017          199751 non-null int64\n",
      "papernum2016          199751 non-null int64\n",
      "papernum2015          199751 non-null int64\n",
      "papernum2014          199751 non-null int64\n",
      "papernum2013          199751 non-null int64\n",
      "num_journal           199751 non-null int64\n",
      "num_conference        199751 non-null int64\n",
      "degree                199470 non-null float64\n",
      "pagerank              199470 non-null float64\n",
      "degree_centrality     199470 non-null float64\n",
      "diff_year             199470 non-null float64\n",
      "coauthors_top10000    199751 non-null int64\n",
      "coauthors_top20000    199751 non-null int64\n",
      "coauthors_top30000    199751 non-null int64\n",
      "category              199751 non-null int64\n",
      "label                 199751 non-null int64\n",
      "dtypes: float64(6), int64(16)\n",
      "memory usage: 33.5 MB\n",
      "data.info(): None\n"
     ]
    }
   ],
   "source": [
    "def get_data(connection):\n",
    "    \"\"\"\n",
    "    查询数据，包括特征和标签\n",
    "    :param connection:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    sql_select = \"\"\"\n",
    "        SELECT teac_id, bys_cn, hindex_cn,a_conf+a_journal as a_paper, b_conf + b_journal as b_paper,c_conf + c_journal as c_paper,papernum2017, papernum2016, papernum2015, papernum2014, papernum2013,num_journal,num_conference, project_num, degree, pagerank,degree_centrality,last_year - first_year as diff_year , coauthors_top10000, coauthors_top20000, coauthors_top30000, category, CASE WHEN label is null THEN 0 ELSE label END label\n",
    "        FROM classifier_isTeacher_xgbc WHERE (label=1 or label=0 or label is null ) and category is not null\n",
    "    \"\"\"\n",
    "    df = pd.read_sql_query(sql_select, connection)\n",
    "    all_features = ['teac_id', 'bys_cn', 'hindex_cn', 'a_paper', 'b_paper', 'c_paper', 'papernum2017', 'papernum2016', 'papernum2015', 'papernum2014', 'papernum2013', 'num_journal', 'num_conference',  'degree', 'pagerank', 'degree_centrality', 'diff_year', 'coauthors_top10000', 'coauthors_top20000', 'coauthors_top30000', 'category', 'label']\n",
    "    data = df[all_features]\n",
    "    return data\n",
    "\n",
    "data = get_data(connection)\n",
    "print(\"shape of data:\", data.shape)\n",
    "print(\"data.info():\", data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 199751 entries, 0 to 199750\n",
      "Data columns (total 22 columns):\n",
      "teac_id               199751 non-null int64\n",
      "bys_cn                199751 non-null float64\n",
      "hindex_cn             199751 non-null float64\n",
      "a_paper               199751 non-null int64\n",
      "b_paper               199751 non-null int64\n",
      "c_paper               199751 non-null int64\n",
      "papernum2017          199751 non-null int64\n",
      "papernum2016          199751 non-null int64\n",
      "papernum2015          199751 non-null int64\n",
      "papernum2014          199751 non-null int64\n",
      "papernum2013          199751 non-null int64\n",
      "num_journal           199751 non-null int64\n",
      "num_conference        199751 non-null int64\n",
      "degree                199751 non-null float64\n",
      "pagerank              199751 non-null float64\n",
      "degree_centrality     199751 non-null float64\n",
      "diff_year             199751 non-null float64\n",
      "coauthors_top10000    199751 non-null int64\n",
      "coauthors_top20000    199751 non-null int64\n",
      "coauthors_top30000    199751 non-null int64\n",
      "category              199751 non-null int64\n",
      "label                 199751 non-null int64\n",
      "dtypes: float64(6), int64(16)\n",
      "memory usage: 33.5 MB\n",
      "info of data:: None\n"
     ]
    }
   ],
   "source": [
    "# 对缺失值进行处理\n",
    "# Method1：直接将含有缺失字段的值去掉\n",
    "# data = data.dropna()\n",
    "# print(\"shape of data::\", data.shape)\n",
    "# print(\"data.info()::\", data.info())\n",
    "columns_name_zero = ['bys_cn', 'hindex_cn', 'degree', 'pagerank', 'degree_centrality', 'diff_year']\n",
    "for column_name in columns_name_zero:\n",
    "    data[column_name].fillna(0, inplace=True)\n",
    "print(\"info of data::\", data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 199751 entries, 0 to 199750\n",
      "Data columns (total 19 columns):\n",
      "bys_cn                199751 non-null float64\n",
      "hindex_cn             199751 non-null float64\n",
      "a_paper               199751 non-null int64\n",
      "b_paper               199751 non-null int64\n",
      "c_paper               199751 non-null int64\n",
      "papernum2017          199751 non-null int64\n",
      "papernum2016          199751 non-null int64\n",
      "papernum2015          199751 non-null int64\n",
      "papernum2014          199751 non-null int64\n",
      "papernum2013          199751 non-null int64\n",
      "num_journal           199751 non-null int64\n",
      "num_conference        199751 non-null int64\n",
      "degree                199751 non-null float64\n",
      "pagerank              199751 non-null float64\n",
      "degree_centrality     199751 non-null float64\n",
      "diff_year             199751 non-null float64\n",
      "coauthors_top10000    199751 non-null int64\n",
      "coauthors_top20000    199751 non-null int64\n",
      "coauthors_top30000    199751 non-null int64\n",
      "dtypes: float64(6), int64(13)\n",
      "memory usage: 29.0 MB\n",
      "info of X_continuous:: None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 199751 entries, 0 to 199750\n",
      "Data columns (total 1 columns):\n",
      "category    199751 non-null int64\n",
      "dtypes: int64(1)\n",
      "memory usage: 1.5 MB\n",
      "info of X_discrete:: None\n",
      "len of ids:: 199751\n",
      "y:: Counter({0: 182766, 1: 16985})\n"
     ]
    }
   ],
   "source": [
    "# 将连续值和离散值以及y分开\n",
    "continuous_features = ['bys_cn', 'hindex_cn', 'a_paper', 'b_paper', 'c_paper', 'papernum2017', 'papernum2016', 'papernum2015', 'papernum2014', 'papernum2013', 'num_journal', 'num_conference',  'degree', 'pagerank', 'degree_centrality', 'diff_year', 'coauthors_top10000', 'coauthors_top20000', 'coauthors_top30000']\n",
    "discrete_features = ['category']\n",
    "X_continous = data[continuous_features]\n",
    "X_discrete = data[discrete_features]\n",
    "y = data['label']\n",
    "ids = data['teac_id']\n",
    "print(\"info of X_continuous::\", X_continous.info())\n",
    "print(\"info of X_discrete::\", X_discrete.info())\n",
    "print(\"len of ids::\", len(ids))\n",
    "print(\"y::\", Counter(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " ...\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "shape of X_all:: (199751, 22)\n"
     ]
    }
   ],
   "source": [
    "# 将离散值变成one-hot编码\n",
    "X_discrete_oneHot = OneHotEncoder(sparse=False).fit_transform(X_discrete)\n",
    "print(X_discrete_oneHot)\n",
    "\n",
    "X_all = np.hstack((X_continous, X_discrete_oneHot))\n",
    "print(\"shape of X_all::\", X_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3、获取标签数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of label_ids:: 3409\n",
      "len of label_labels:: 3409\n"
     ]
    }
   ],
   "source": [
    "def get_ground_truth(connection):\n",
    "    \"\"\"\n",
    "    获取标签数据，这部分主要是训练的时候使用的是前17605个正例，所以这里验证使用id为17605之后的数据和负例数据\n",
    "    \"\"\"\n",
    "    sql_select_zeros = \"SELECT teac_id, label FROM classifier_isTeacher_label WHERE label = 0 and category is not null\"\n",
    "    sql_select_ones = \"SELECT teac_id, label FROM classifier_isTeacher_label WHERE label = 1 and teac_id>17606 and category is not null ORDER BY xmpy LIMIT 10, 1700\"\n",
    "    cursor = connection.cursor()\n",
    "    ids = []\n",
    "    labels =[]\n",
    "    cursor.execute(sql_select_zeros)\n",
    "    results1 = cursor.fetchall() \n",
    "    for elem in results1:\n",
    "        ids.append(elem[0])\n",
    "        labels.append(elem[1])\n",
    "    cursor.execute(sql_select_ones)\n",
    "    results2 = cursor.fetchall() \n",
    "    for elem in results2:\n",
    "        ids.append(elem[0])\n",
    "        labels.append(elem[1])\n",
    "    return ids, labels\n",
    "\n",
    "label_ids, label_labels = get_ground_truth(connection)\n",
    "print(\"len of label_ids::\", len(label_ids))\n",
    "print(\"len of label_labels::\", len(label_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4、评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_evaluaton(ids, predict, label_ids, label_labels):\n",
    "    \"\"\"\n",
    "    根据预测结果和ground-truth评估性能。\n",
    "    ids: 所有人的id\n",
    "    predict：所有人的预测结果\n",
    "    label_ids：用于衡量性能的ground-truth的id\n",
    "    label_labels：用于衡量性能的ground-truth的标签\n",
    "    \"\"\"\n",
    "    predict_labels = []    # 存储对于ground_truth中的id，预测结果\n",
    "    res_map = {}\n",
    "    predict_lis = []    # 存储\n",
    "    for i in range(0, len(ids)):\n",
    "        if isnan(predict[i]):\n",
    "            res_map[ids[i]] = 1\n",
    "            predict_lis.append(1)\n",
    "        else:\n",
    "            if predict[i] > 0.6:   # 定义阈值为0.6\n",
    "                res_map[ids[i]] = 1\n",
    "                predict_lis.append(1)\n",
    "            else:\n",
    "                res_map[ids[i]] = 0\n",
    "                predict_lis.append(0)\n",
    "    consistent = 0\n",
    "    consistent_zeros = 0\n",
    "    for i in range(0, len(label_ids)):\n",
    "        predict_label = res_map.get(label_ids[i])\n",
    "        predict_labels.append(predict_label)\n",
    "        if predict_label== label_labels[i]:\n",
    "            consistent += 1\n",
    "            if label_labels[i] == 0:\n",
    "                consistent_zeros += 1\n",
    "#     accuracy = consistent / len(label_ids)\n",
    "    print(\"consistent::%d\" % consistent)\n",
    "    print(\"correct of consistent_zeros::%d\" % consistent_zeros)\n",
    "    return consistent, consistent_zeros, predict_labels, predict_lis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5、PU-Learning\n",
    "\n",
    "### 5.1 pu-bagging方法\n",
    "\n",
    "pu-bagging借助了bagging的思想，步骤如下：\n",
    "\n",
    "（1）采样与正例相同大小的无标签数据当做负样本\n",
    "\n",
    "（2）使用正例和负例训练分类器，预测除此正例和负例之外的数据标签\n",
    "\n",
    "（3）重复多次，取预测的平均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of features::15\n",
      "Ranking of features:: [1 1 4 1 1 3 1 1 1 1 1 1 1 1 7 1 5 6 1 1 8 2]\n",
      "selector.support_:: [ True  True False  True  True False  True  True  True  True  True  True\n",
      "  True  True False  True False False  True  True False False]\n",
      "type of X_continuous_new:: <class 'numpy.ndarray'>\n",
      "shape of X_continuous_new:: (199751, 13)\n",
      "shape of X_all:: (199751, 22)\n",
      "[nan nan nan ...  1.  0. nan]\n",
      "结果为Nan的元素是：：33191\n",
      "结果为1的元素是：：74227\n",
      "总长度是：：199751\n",
      "consistent::3101\n",
      "correct of consistent_zeros::1401\n",
      "consistent::3101\n",
      "correct of consistent_zeros::1401\n",
      "准确率是： 0.9096509240246407\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      1.00      0.90      1401\n",
      "           1       1.00      0.85      0.92      2008\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      3409\n",
      "   macro avg       0.91      0.92      0.91      3409\n",
      "weighted avg       0.93      0.91      0.91      3409\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 通过观察可以发现，使用rfe方法，当n_features_to_select=15时，f1值可以达到最大值：0.96038，这也是方差分析，rfe和rfecv中最好的效果。\n",
    "def train_and_test_XGBC_rfe_puBagging(X_all, y, ids, label_ids, label_labels, n_features_to_select=15):\n",
    "    \n",
    "     # RFECV\n",
    "    estimator = XGBClassifier()\n",
    "    selector = RFE(estimator=estimator, n_features_to_select=n_features_to_select)\n",
    "    X_all_rfe = selector.fit_transform(X_all, y) \n",
    "    print(\"Optimal number of features::%d\" % selector.n_features_)\n",
    "    print(\"Ranking of features:: %s\" % selector.ranking_)\n",
    "    \n",
    "    selected_idx = np.where(pd.Series(selector.support_)==True)[0]   # n_features_to_select个选择出来的特征，每一个特征为True\n",
    "    print(\"selector.support_::\", selector.support_)\n",
    "    \n",
    "    # 因为Wrapper离散特征和连续特征需要一起训练搜索特征子集，但是因为离散特征不需要标准化，所以这里需要将其分开\n",
    "    discrete_idx = list(set([19, 20, 21]) - set(selected_idx))   # 最后3列为离散值\n",
    "    X_continuous_tmp = pd.DataFrame(X_all_rfe)[list(range(0, len(selected_idx)-len(discrete_idx)))]\n",
    "    X_discreate_tmp = pd.DataFrame(X_all_rfe)[list(range(len(selected_idx)-len(discrete_idx), len(selected_idx)))]\n",
    "    \n",
    "    # 归一化\n",
    "    ss = StandardScaler()\n",
    "    X_continuous_new = ss.fit_transform(X_continuous_tmp)\n",
    "    print(\"type of X_continuous_new::\", type(X_continuous_new))\n",
    "    print(\"shape of X_continuous_new::\", X_continuous_new.shape)\n",
    "\n",
    "    # 将连续值和离散值拼接\n",
    "    X_all_new = np.hstack((X_continuous_new, X_discreate_tmp))\n",
    "    print(\"shape of X_all::\", X_all.shape)\n",
    "    \n",
    "    y_origin = y.copy()\n",
    "\n",
    "    bc = BaggingClassifierPU(\n",
    "        DecisionTreeClassifier(),\n",
    "        n_estimators=1,  # 1000 trees as usual\n",
    "        max_samples=sum(y),  # Balance the positives and unlabeled in each bag\n",
    "    )\n",
    "    bc.fit(X_all_new, y)\n",
    "    \n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators = 1,  # Use 1000 trees\n",
    "    )\n",
    "    rf.fit(X_all_new, y)\n",
    "    \n",
    "    # Store the scores assigned by this approach\n",
    "    results = pd.DataFrame({\n",
    "        'truth'      : y_origin,   # The true labels\n",
    "        'label'      : y,        # The labels to be shown to models in experiment\n",
    "        'output_std' : rf.predict_proba(X_all_new)[:,1]   # The random forest's scores\n",
    "    }, columns = ['truth', 'label', 'output_std'])\n",
    "\n",
    "    results['output_skb'] = bc.oob_decision_function_[:, 1]\n",
    "    print(bc.oob_decision_function_[:, 1])\n",
    "    res = bc.oob_decision_function_[:, 1]\n",
    "    count_Nan = 0\n",
    "    count_one = 0\n",
    "    for i in range(0, len(res)):\n",
    "        if isnan(res[i]):\n",
    "            count_Nan += 1\n",
    "            count_one += 1\n",
    "        if res[i] > 0.6:\n",
    "            count_one += 1\n",
    "    print(\"结果为Nan的元素是：：%d\" % count_Nan)\n",
    "    print(\"结果为1的元素是：：%d\" % count_one)\n",
    "    print(\"总长度是：：%d\" % len(res))\n",
    "    consistent, consistent_zeros, predict_labels, predict_lis = get_evaluaton(ids, res, label_ids, label_labels)\n",
    "    print(\"consistent::%d\" %consistent)\n",
    "    print(\"correct of consistent_zeros::%d\" % consistent_zeros)\n",
    "    if len(predict_labels) != len(label_labels):\n",
    "        print(\"真实值和预测值的长度不同\")\n",
    "#     print(\"predict_labels::\", predict_labels)\n",
    "#     print(\"type of predict_labels::\", type(predict_labels))\n",
    "#     print(\"label_labels::\", label_labels)\n",
    "#     print(\"type of label_labels::\", type(label_labels))\n",
    "    score = len(np.where((pd.Series(predict_labels) == pd.Series(label_labels)) == True)[0])/len(predict_labels)\n",
    "    print(\"准确率是：\", score)\n",
    "    print(classification_report(predict_labels, label_labels))   # target_names=['1', '0']\n",
    "\n",
    "# 调用预测函数\n",
    "X_all_copy = X_all.copy()\n",
    "y_copy = y.copy()\n",
    "train_and_test_XGBC_rfe_puBagging(X_all_copy, y_copy, ids, label_ids, label_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Two-step\n",
    "\n",
    "two-step的思想如下：\n",
    "\n",
    "（1）首先将所有的无标签数据当做负样本，和所有正例当做训练集训练分类器，识别出无标签样本数据中可靠的负例，将其当做真正的负例。\n",
    "\n",
    "（2）使用正例和Step1中的可靠负例训练分类器，在挑选中可靠负例，不但迭代（本次实验迭代了10次）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of features::15\n",
      "Ranking of features:: [1 1 4 1 1 3 1 1 1 1 1 1 1 1 7 1 5 6 1 1 8 2]\n",
      "selector.support_:: [ True  True False  True  True False  True  True  True  True  True  True\n",
      "  True  True False  True False False  True  True False False]\n",
      "type of X_continuous_new:: <class 'numpy.ndarray'>\n",
      "shape of X_continuous_new:: (199751, 13)\n",
      "shape of X_all:: (199751, 22)\n",
      "Step 1 labeled 0 new positives and 139733 new negatives.\n",
      "Doing step 2... Step 1 labeled 0 new positives and 139480 new negatives.\n",
      "Doing step 2... Step 1 labeled 0 new positives and 139613 new negatives.\n",
      "Doing step 2... Step 1 labeled 0 new positives and 139752 new negatives.\n",
      "Doing step 2... Step 1 labeled 0 new positives and 139635 new negatives.\n",
      "Doing step 2... Step 1 labeled 0 new positives and 139850 new negatives.\n",
      "Doing step 2... Step 1 labeled 0 new positives and 139980 new negatives.\n",
      "Doing step 2... Step 1 labeled 0 new positives and 139638 new negatives.\n",
      "Doing step 2... Step 1 labeled 0 new positives and 139821 new negatives.\n",
      "Doing step 2... Step 1 labeled 0 new positives and 139469 new negatives.\n",
      "Doing step 2... pred:: [0.8 1.  0.9 ... 0.2 0.1 0.7]\n",
      "结果为Nan的元素是：：0\n",
      "结果为1的元素是：：12913\n",
      "总长度是：：199751\n",
      "consistent::2839\n",
      "correct of consistent_zeros::1709\n",
      "consistent::2839\n",
      "correct of consistent_zeros::1709\n",
      "准确率是： 0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.75      0.86      2279\n",
      "           1       0.66      1.00      0.80      1130\n",
      "\n",
      "   micro avg       0.83      0.83      0.83      3409\n",
      "   macro avg       0.83      0.87      0.83      3409\n",
      "weighted avg       0.89      0.83      0.84      3409\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 通过观察可以发现，使用rfe方法，当n_features_to_select=15时，f1值可以达到最大值：0.96038，这也是方差分析，rfe和rfecv中最好的效果。\n",
    "def train_and_test_XGBC_rfe_puTwoStep(X_all, y, ids, label_ids, label_labels, n_features_to_select=15):\n",
    "    \n",
    "     # RFECV\n",
    "    estimator = XGBClassifier()\n",
    "    selector = RFE(estimator=estimator, n_features_to_select=n_features_to_select)\n",
    "    X_all_rfe = selector.fit_transform(X_all, y) \n",
    "    print(\"Optimal number of features::%d\" % selector.n_features_)\n",
    "    print(\"Ranking of features:: %s\" % selector.ranking_)\n",
    "    \n",
    "    selected_idx = np.where(pd.Series(selector.support_)==True)[0]   # n_features_to_select个选择出来的特征，每一个特征为True\n",
    "    print(\"selector.support_::\", selector.support_)\n",
    "    \n",
    "    # 因为Wrapper离散特征和连续特征需要一起训练搜索特征子集，但是因为离散特征不需要标准化，所以这里需要将其分开\n",
    "    discrete_idx = list(set([19, 20, 21]) - set(selected_idx))   # 最后3列为离散值\n",
    "    X_continuous_tmp = pd.DataFrame(X_all_rfe)[list(range(0, len(selected_idx)-len(discrete_idx)))]\n",
    "    X_discreate_tmp = pd.DataFrame(X_all_rfe)[list(range(len(selected_idx)-len(discrete_idx), len(selected_idx)))]\n",
    "    \n",
    "    # 归一化\n",
    "    ss = StandardScaler()\n",
    "    X_continuous_new = ss.fit_transform(X_continuous_tmp)\n",
    "    print(\"type of X_continuous_new::\", type(X_continuous_new))\n",
    "    print(\"shape of X_continuous_new::\", X_continuous_new.shape)\n",
    "\n",
    "    # 将连续值和离散值拼接\n",
    "    X_all_new = np.hstack((X_continuous_new, X_discreate_tmp))\n",
    "    print(\"shape of X_all::\", X_all.shape)\n",
    "    \n",
    "    # Get the scores from RandomForestClassifier\n",
    "    rf = RandomForestClassifier(n_estimators = 10)   # Use 1000 trees\n",
    "    rf.fit(X_all_new, y)\n",
    "    pred = rf.predict_proba(X_all_new)[:,1]\n",
    "    \n",
    "    # Find the range of scores given to positive data points\n",
    "    range_P = [min(pred * (y == 1)), max(pred * (y == 1))]\n",
    "\n",
    "    # STEP 1\n",
    "    # If any unlabeled point has a score above all known positives, \n",
    "    # or below all known positives, label it accordingly\n",
    "    iP_new = y[(y == 0) & (pred >= range_P[1])].index\n",
    "    iN_new = y[(y == 0) & (pred <= range_P[0])].index\n",
    "    y.loc[iP_new] = 1\n",
    "    y.loc[iN_new] = 0\n",
    "    \n",
    "    \n",
    "    # Classifier to be used for step 2\n",
    "    rf2 = RandomForestClassifier(n_estimators = 10)\n",
    "\n",
    "    # Limit to 10 iterations (this is arbitrary, but \n",
    "    # otherwise this approach can take a very long time)\n",
    "    for i in range(10):\n",
    "        # If step 1 didn't find new labels, we're done\n",
    "        if len(iP_new) + len(iN_new) == 0 and i > 0:\n",
    "            break\n",
    "\n",
    "        print('Step 1 labeled %d new positives and %d new negatives.' % (len(iP_new), len(iN_new)))\n",
    "        print('Doing step 2... ', end = '')\n",
    "\n",
    "        # STEP 2\n",
    "        # Retrain on new labels and get new scores\n",
    "        rf2.fit(X_all_new, y)\n",
    "        pred = rf2.predict_proba(X_all_new)[:,-1]\n",
    "\n",
    "        # Find the range of scores given to positive data points\n",
    "        range_P = [min(pred * (y == 1)), max(pred * (y == 1))]\n",
    "\n",
    "        # Repeat step 1\n",
    "        iP_new = y[(y == 0) & (pred >= range_P[1])].index\n",
    "        iN_new = y[(y == 0) & (pred <= range_P[0])].index\n",
    "        y.loc[iP_new] = 1\n",
    "        y.loc[iN_new] = 0\n",
    "\n",
    "\n",
    "    # Lastly, get the scores assigned by this approach    \n",
    "    print(\"pred::\", pred)\n",
    "    res = pred\n",
    "    count_Nan = 0\n",
    "    count_one = 0\n",
    "    for i in range(0, len(res)):\n",
    "        if isnan(res[i]):\n",
    "            count_Nan += 1\n",
    "            count_one += 1\n",
    "        if res[i] > 0.6:\n",
    "            count_one += 1\n",
    "    print(\"结果为Nan的元素是：：%d\" % count_Nan)\n",
    "    print(\"结果为1的元素是：：%d\" % count_one)\n",
    "    print(\"总长度是：：%d\" % len(res))\n",
    "    consistent, consistent_zeros, predict_labels, predict_lis = get_evaluaton(ids, res, label_ids, label_labels)\n",
    "    print(\"consistent::%d\" %consistent)\n",
    "    print(\"correct of consistent_zeros::%d\" % consistent_zeros)\n",
    "    if len(predict_labels) != len(label_labels):\n",
    "        print(\"真实值和预测值的结果不同\")\n",
    "    score = len(np.where((predict_labels == label_labels) == True)[0])/len(predict_labels)\n",
    "    print(\"准确率是：\", score)\n",
    "    print(classification_report(predict_labels, label_labels))   # target_names=['1', '0']\n",
    "\n",
    "# 调用预测函数\n",
    "X_all_copy = X_all.copy()\n",
    "y_copy = y.copy()\n",
    "train_and_test_XGBC_rfe_puTwoStep(X_all_copy, y_copy, ids, label_ids, label_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分析\n",
    "\n",
    "通过上述结果发现，使用pu-bagging的效果要稍微好于使用two-step的效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
